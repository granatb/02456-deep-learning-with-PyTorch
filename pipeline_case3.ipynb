{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pipeline_case3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "363.391px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/granatb/02456-deep-learning-with-PyTorch/blob/master/pipeline_case3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:27.362716Z",
          "start_time": "2021-11-22T21:29:27.348023Z"
        },
        "id": "c-Dz3WXtUCXN"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.418536Z",
          "start_time": "2021-11-22T21:29:27.364711Z"
        },
        "id": "2SSNpY9L4FEd",
        "scrolled": true
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import torch #pytorch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable \n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.434423Z",
          "start_time": "2021-11-22T21:29:29.420528Z"
        },
        "id": "JmtFI5uh4Nqp"
      },
      "source": [
        "def set_pandas_display_options() -> None:\n",
        "    \"\"\"Set pandas display options.\"\"\"\n",
        "    # Ref: https://stackoverflow.com/a/52432757/\n",
        "    display = pd.options.display\n",
        "\n",
        "    display.max_columns = 1000\n",
        "    display.max_rows = 1000\n",
        "    display.max_colwidth = 199\n",
        "    display.width = 1000\n",
        "    # display.precision = 2  # set as needed\n",
        "\n",
        "set_pandas_display_options()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.449381Z",
          "start_time": "2021-11-22T21:29:29.435526Z"
        },
        "id": "wmNvd-TU4PCH"
      },
      "source": [
        "def Lag_N(series, N):\n",
        "    return series.shift(N)\n",
        "\n",
        "def Lag_diff(series, N):\n",
        "    return series.diff(periods = N)\n",
        "\n",
        "def Lag_ratio(series, N):\n",
        "    return series/series.shift(N)\n",
        "\n",
        "def AVG_N(series, N):\n",
        "    ret = [0]*len(series)\n",
        "    for i in range(N,len(series)):\n",
        "        ret[i] = np.mean(series[i-N:i])\n",
        "    return ret\n",
        "\n",
        "def Min_N(series, N):\n",
        "    ret = [0]*len(series)\n",
        "    for i in range(N, len(series)):\n",
        "        ret[i] = np.min(series[i-N:i])\n",
        "    return ret\n",
        "\n",
        "def Max_N(series, N):\n",
        "    ret = [0]*len(series)\n",
        "    for i in range(N, len(series)):\n",
        "        ret[i] = np.max(series[i-N:i])\n",
        "    return ret\n",
        "\n",
        "def Avg_shift(series, rng):\n",
        "    ret = [0]*len(series)\n",
        "    for i in range(rng[1], len(series)):\n",
        "        ret[i] = np.mean(series[i-rng[1]:i-rng[0]+1])\n",
        "    return ret\n",
        "def SD_N(series, N):\n",
        "    ret = [0] * len(series)\n",
        "    for i in range(N, len(series)):\n",
        "        ret[i] = np.std(series[i - N:i])\n",
        "    return ret\n",
        "\n",
        "def EMA_N(series, N):\n",
        "    ret = pd.Series.ewm(series, span = N, adjust = False).mean()\n",
        "    return ret\n",
        "\n",
        "def MACD(series, args):\n",
        "    ema_1 = pd.Series.ewm(series, span = args[0], adjust = False).mean()\n",
        "    ema_2 = pd.Series.ewm(series, span = args[1], adjust = False).mean()\n",
        "    ema_signal = pd.Series.ewm(series, span = args[2], adjust = False).mean()\n",
        "    ret = ema_1-ema_2-ema_signal\n",
        "    return ret\n",
        "\n",
        "def SO_N(series, N):\n",
        "    ret = [0] * len(series)\n",
        "    for i in range(N, len(series)):\n",
        "        L_N = np.min(series[i-N:i])\n",
        "        H_N = np.max(series[i-N:i])\n",
        "        ret[i] = 100*(series[i-1]-L_N)/(H_N-L_N)\n",
        "    return ret\n",
        "    \n",
        "def RSI_N(series, N):\n",
        "    delta = series.diff()\n",
        "    up, down = delta.copy(), delta.copy()\n",
        "    up[up < 0] = 0\n",
        "    down[down > 0] = 0\n",
        "    avg_gain = [0] * len(series)\n",
        "    avg_loss = [0] * len(series)\n",
        "    avg_gain[N+1] = pd.Series(up[1:N+1]).mean()\n",
        "    avg_loss[N+1] = pd.Series(down[1:N+1]).mean()\n",
        "    for i in range(N+2, len(series)):\n",
        "        avg_gain[i] = ((N-1)*avg_gain[i-1]+up[i-1])/N\n",
        "        avg_loss[i] = ((N-1) * avg_loss[i-1]+down[i-1]) / N\n",
        "\n",
        "    RS = pd.Series(avg_gain) / ((-1) * pd.Series(avg_loss))\n",
        "    RSI = 100.0 - (100.0 / (1.0 + RS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:33:50.228618Z",
          "start_time": "2021-11-22T21:33:50.212637Z"
        },
        "id": "JWd9kW6r4Pnl"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.950186Z",
          "start_time": "2021-11-22T21:29:29.950186Z"
        },
        "id": "pmZCvTBv466u"
      },
      "source": [
        "datasets_path = \"/content/drive/My Drive/data/Datasets/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.951183Z",
          "start_time": "2021-11-22T21:29:29.951183Z"
        },
        "id": "8xfG3_9gw5LI"
      },
      "source": [
        "df = pd.read_csv(datasets_path+'Case3/Proj_NWP_Case3.csv')\n",
        "df.Date_Time = pd.to_datetime(df.Date_Time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.952179Z",
          "start_time": "2021-11-22T21:29:29.952179Z"
        },
        "id": "t_RVe6jPw5LJ"
      },
      "source": [
        "dfm = pd.read_csv(datasets_path+'Case3/Proj_Measurements_Case3.csv')\n",
        "dfm.Date_Time = pd.to_datetime(dfm.Date_Time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.953197Z",
          "start_time": "2021-11-22T21:29:29.953197Z"
        },
        "id": "giOHUe0b5FWq"
      },
      "source": [
        "# df_full = dfm.add_suffix('_m').join(df.add_suffix('_nwp'))\n",
        "df_full = dfm.merge(df, on = \"Date_Time\", suffixes=('_m', '_nwp'))\n",
        "df_full['Month'] = pd.DatetimeIndex(df_full.Date_Time).month\n",
        "df_full['Year'] = pd.DatetimeIndex(df_full.Date_Time).year\n",
        "df_full['Day'] = pd.DatetimeIndex(df_full.Date_Time).day\n",
        "df_full.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.954695Z",
          "start_time": "2021-11-22T21:29:29.954695Z"
        },
        "id": "U5ZV-De5M3YW"
      },
      "source": [
        "target_variable = ['Park Power [KW]']\n",
        "model_cols = ['Date_Time', 'Month', 'Year', 'Speed_100m', 'Direction_100m','Air Density_100m','Speed_50m_nwp', 'Direction_50m_nwp','Air Density_50m','Speed_10m_nwp','Speed_150m']\n",
        "final_cols = ['Speed_100m', 'Direction_100m','Speed_150m','Speed_50m_nwp', 'Direction_50m_nwp','Air Density_50m','Speed_10m_nwp']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.954695Z",
          "start_time": "2021-11-22T21:29:29.954695Z"
        },
        "id": "VPMiBJprM3YW"
      },
      "source": [
        "df_model = df_full[target_variable+model_cols]\n",
        "df_model.dropna(inplace=True)\n",
        "df_model = df_model[~(df_model['Park Power [KW]'] < 0)]\n",
        "df_model = df_model[~((df_model.Speed_100m > 4) & (df_model['Park Power [KW]'] == 0))]\n",
        "df_model = df_model[~((df_model.Speed_10m_nwp > 4) & (df_model['Park Power [KW]'] == 0))]\n",
        "df_model = df_model[~((df_model.Speed_50m_nwp > 4) & (df_model['Park Power [KW]'] == 0))]\n",
        "df_model['Power_SD_12'] = SD_N(df_model['Park Power [KW]'], 12)\n",
        "df_model.loc[df_model.Speed_100m > 12, 'Speed_100m'] = 12\n",
        "df_model.loc[df_model.Speed_10m_nwp > 7.12, 'Speed_10m_nwp'] = 7.12\n",
        "df_model.loc[df_model.Speed_50m_nwp > 6.16, 'Speed_50m'] = 6.16\n",
        "df_model = df_model[~(df_model['Power_SD_12'] == 0)]\n",
        "df_model.set_index('Date_Time', inplace = True)\n",
        "df_model = df_model.loc[:, target_variable+final_cols]\n",
        "df_model.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.955687Z",
          "start_time": "2021-11-22T21:29:29.955687Z"
        },
        "id": "Rxdp55GrM3YW"
      },
      "source": [
        "df_model.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.956685Z",
          "start_time": "2021-11-22T21:29:29.956685Z"
        },
        "id": "YwTUD2vNM3YX"
      },
      "source": [
        "y = df_model['Park Power [KW]']\n",
        "X = df_model[final_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.956685Z",
          "start_time": "2021-11-22T21:29:29.956685Z"
        },
        "id": "XOwvUzBxu5s6"
      },
      "source": [
        "sequence_length = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.957682Z",
          "start_time": "2021-11-22T21:29:29.957682Z"
        },
        "id": "3NHjs_LcM3YX"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "mm = MinMaxScaler()\n",
        "ss = StandardScaler()\n",
        "\n",
        "\n",
        "X_ss = ss.fit_transform(X)\n",
        "y_mm = mm.fit_transform(y.to_numpy().reshape(-1, 1))\n",
        "y_mm = y_mm[sequence_length:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.958679Z",
          "start_time": "2021-11-22T21:29:29.958679Z"
        },
        "id": "G82y3lRwRUl9"
      },
      "source": [
        "X_seq = []\n",
        "for i in range(0, X.shape[0]):\n",
        "    if i >= sequence_length:\n",
        "        i_start = i - sequence_length + 1\n",
        "        x = X_ss[i_start:(i + 1), :]\n",
        "        X_seq.append(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.959678Z",
          "start_time": "2021-11-22T21:29:29.959678Z"
        },
        "id": "RjaXWDedTKBu"
      },
      "source": [
        "X_seq = np.asarray(X_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.959678Z",
          "start_time": "2021-11-22T21:29:29.959678Z"
        },
        "id": "msDYvOEvihXa"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# def train_val_test_split(X, y, target_col, test_ratio):\n",
        "#     val_ratio = test_ratio / (1 - test_ratio)\n",
        "#     # X, y = feature_label_split(df, target_col)\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)\n",
        "#     X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_ratio, shuffle=False)\n",
        "#     return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X_seq, y_mm, 'value', 0.2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_mm, test_size=0.2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.960675Z",
          "start_time": "2021-11-22T21:29:29.960675Z"
        },
        "id": "YgjNwfK_M3YX"
      },
      "source": [
        "print(\"Training Shape\", X_train.shape, y_train.shape)\n",
        "print(\"Testing Shape\", X_test.shape, y_test.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.962668Z",
          "start_time": "2021-11-22T21:29:29.962668Z"
        },
        "id": "Qsd8YlPHM3YY"
      },
      "source": [
        "X_train_tensors = Variable(torch.Tensor(X_train))\n",
        "X_test_tensors = Variable(torch.Tensor(X_test))\n",
        "\n",
        "y_train_tensors = Variable(torch.Tensor(y_train))\n",
        "y_test_tensors = Variable(torch.Tensor(y_test)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.962668Z",
          "start_time": "2021-11-22T21:29:29.962668Z"
        },
        "id": "KsPpm_P8kOV6"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train = TensorDataset(X_train_tensors, y_train_tensors)\n",
        "test = TensorDataset(X_test_tensors, y_test_tensors)\n",
        "\n",
        "# train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "test_loader_one = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.963665Z",
          "start_time": "2021-11-22T21:29:29.963665Z"
        },
        "id": "Ns500JukE4SW"
      },
      "source": [
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.964662Z",
          "start_time": "2021-11-22T21:29:29.964662Z"
        },
        "id": "jrn4Y7QrM3YY"
      },
      "source": [
        "class LSTM1(nn.Module):\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
        "        super(LSTM1, self).__init__()\n",
        "        self.num_classes = num_classes #number of classes\n",
        "        self.num_layers = num_layers #number of layers\n",
        "        self.input_size = input_size #input size\n",
        "        self.hidden_size = hidden_size #hidden state\n",
        "        self.seq_length = seq_length #sequence length\n",
        "        linear_out_size = 64\n",
        "\n",
        "        # self.linear_out =  nn.Linear(input_size, linear_out_size) #fully connected 1\n",
        "\n",
        "        self.fnn = nn.Sequential(\n",
        "            nn.Linear(input_size, linear_out_size, bias=False),\n",
        "            nn.BatchNorm1d(linear_out_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(linear_out_size, linear_out_size, bias=False)\n",
        "        )\n",
        "\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                          num_layers=num_layers, batch_first=True) #lstm\n",
        "        # self.fc_1 =  nn.Linear(hidden_size+linear_out_size, 256) #fully connected 1\n",
        "        self.fc_1 = nn.Sequential(\n",
        "            nn.Linear(hidden_size+linear_out_size, 256, bias=False),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 256, bias=False)\n",
        "        )\n",
        "        self.fc = nn.Linear(256, num_classes) #fully connected last layer\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self,x):\n",
        "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #hidden state\n",
        "        # print(h_0.shape)\n",
        "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #internal state\n",
        "        # Propagate input through LSTM\n",
        "        # lin_out = self.linear_out(x[:,-1])\n",
        "        lin_out = self.fnn(x[:,-1])\n",
        "        lin_out = self.relu(lin_out)\n",
        "        # print(lin_out.shape)\n",
        "        # print(x.shape)\n",
        "        output, (hn, cn) = self.lstm(x.to(device), (h_0.to(device), c_0.to(device))) #lstm with input, hidden, and internal state\n",
        "        # print(hn.shape)\n",
        "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
        "        out = self.relu(hn)\n",
        "        # print(out.shape)\n",
        "        out =  torch.cat((out, lin_out),1)\n",
        "        # print(out.shape)\n",
        "        out = self.fc_1(out) #first Dense\n",
        "        out = self.relu(out) #relu\n",
        "        out = self.fc(out) #Final Output\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.964662Z",
          "start_time": "2021-11-22T21:29:29.964662Z"
        },
        "id": "GoRSh2BVlITU"
      },
      "source": [
        "class Optimization:\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "    \n",
        "    def train_step(self, x, y):\n",
        "        # Sets model to train mode\n",
        "        self.model.train()\n",
        "\n",
        "        # Makes predictions\n",
        "        yhat = self.model(x)\n",
        "\n",
        "        # Computes loss\n",
        "        loss = self.loss_fn(y, yhat)\n",
        "\n",
        "        # Computes gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Updates parameters and zeroes gradients\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Returns the loss\n",
        "        return loss.item()\n",
        "\n",
        "    def train(self, train_loader, val_loader, batch_size=64, n_epochs=50, n_features=1):\n",
        "        model_path = f'models/{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
        "          # Define the K-fold Cross Validator\n",
        " \n",
        "\n",
        "\n",
        "        for epoch in range(1, n_epochs + 1):\n",
        "            batch_losses = []\n",
        "            for x_batch, y_batch in train_loader:\n",
        "                x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "                loss = self.train_step(x_batch, y_batch)\n",
        "                batch_losses.append(loss)\n",
        "            training_loss = np.mean(batch_losses)\n",
        "            self.train_losses.append(training_loss)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                batch_val_losses = []\n",
        "                for x_val, y_val in val_loader:\n",
        "                    x_val = x_val.view([batch_size, -1, n_features]).to(device)\n",
        "                    y_val = y_val.to(device)\n",
        "                    self.model.eval()\n",
        "                    yhat = self.model(x_val)\n",
        "                    val_loss = self.loss_fn(y_val, yhat).item()\n",
        "                    batch_val_losses.append(val_loss)\n",
        "                validation_loss = np.mean(batch_val_losses)\n",
        "                self.val_losses.append(validation_loss)\n",
        "\n",
        "            if (epoch <= 10) | (epoch % 50 == 0):\n",
        "                print(\n",
        "                    f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
        "                )\n",
        "\n",
        "            # torch.save(self.model.state_dict(), model_path)\n",
        "\n",
        "\n",
        "    def evaluate(self, test_loader, batch_size=1, n_features=1):\n",
        "        with torch.no_grad():\n",
        "            predictions = []\n",
        "            values = []\n",
        "            for x_test, y_test in test_loader:\n",
        "                x_test = x_test.view([batch_size, -1, n_features]).to(device)\n",
        "                y_test = y_test.to(device)\n",
        "                self.model.eval()\n",
        "                yhat = self.model(x_test)\n",
        "                predictions.append(yhat.cpu().numpy()) #.to(device).detach()\n",
        "                values.append(y_test.cpu().numpy())\n",
        "\n",
        "        return predictions, values\n",
        "\n",
        "    def plot_losses(self):\n",
        "        plt.plot(self.train_losses, label=\"Training loss\")\n",
        "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
        "        plt.legend()\n",
        "        plt.title(\"Losses\")\n",
        "        plt.show()\n",
        "        plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.965660Z",
          "start_time": "2021-11-22T21:29:29.965660Z"
        },
        "id": "vnNz4S58nPlI"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.966659Z",
          "start_time": "2021-11-22T21:29:29.966659Z"
        },
        "id": "tyvItgHglno1"
      },
      "source": [
        "import torch.optim as optim\n",
        "from datetime import datetime\n",
        "input_dim = 7 #len(X_train.columns)\n",
        "output_dim = 1\n",
        "hidden_dim = 64\n",
        "layer_dim = 5\n",
        "batch_size = 64\n",
        "dropout = 0.2\n",
        "n_epochs = 100\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1e-6\n",
        "\n",
        "model_params = {'input_dim': input_dim,\n",
        "                'hidden_dim' : hidden_dim,\n",
        "                'layer_dim' : layer_dim,\n",
        "                'output_dim' : output_dim,\n",
        "                'batch_size':batch_size}\n",
        "\n",
        "input_size = 7 #number of features\n",
        "hidden_size = 64 #number of features in hidden state\n",
        "num_layers = 1 #number of stacked lstm layers\n",
        "\n",
        "num_classes = 1 #number of output classes \n",
        "cv = False\n",
        "\n",
        "\n",
        "if cv:\n",
        "\n",
        "    kfold = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "        # K-fold Cross Validation model evaluation\n",
        "    for fold, (train_ids, test_ids) in enumerate(kfold.split(train)):\n",
        "        \n",
        "        # Print\n",
        "        print(f'FOLD {fold}')\n",
        "        print('--------------------------------')\n",
        "        \n",
        "        model = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1]).to(device) #our lstm class \n",
        "\n",
        "        # for layer in self.model.children():\n",
        "        #     if hasattr(layer, 'reset_parameters'):\n",
        "        #         layer.reset_parameters()\n",
        "\n",
        "\n",
        "        # Sample elements randomly from a given list of ids, no replacement.\n",
        "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "        \n",
        "        # Define data loaders for training and testing data in this fold\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "                        train, \n",
        "                        batch_size=64, sampler=train_subsampler, drop_last=True)\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "                        train,\n",
        "                        batch_size=64, sampler=test_subsampler, drop_last = True)   \n",
        "        \n",
        "        loss_fn = nn.MSELoss(reduction=\"mean\")\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "        opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
        "        opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)\n",
        "        opt.plot_losses()\n",
        "\n",
        "        predictions, values = opt.evaluate(test_loader_one, batch_size=1, n_features=input_dim)\n",
        "\n",
        "else:\n",
        "\n",
        "    model = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1]).to(device) #our lstm class \n",
        "\n",
        "    # for layer in self.model.children():\n",
        "    #     if hasattr(layer, 'reset_parameters'):\n",
        "    #         layer.reset_parameters()\n",
        "\n",
        "\n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(list(range(80000)))\n",
        "    test_subsampler = torch.utils.data.SubsetRandomSampler(list(range(80000, len(train))))\n",
        "    \n",
        "    # Define data loaders for training and testing data in this fold\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "                    train, \n",
        "                    batch_size=64, sampler=train_subsampler, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "                    train,\n",
        "                    batch_size=64, sampler=test_subsampler, drop_last = True)   \n",
        "    \n",
        "    loss_fn = nn.MSELoss(reduction=\"mean\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
        "    opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)\n",
        "    opt.plot_losses()\n",
        "\n",
        "    predictions, values = opt.evaluate(test_loader_one, batch_size=1, n_features=input_dim)\n",
        "# model = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1]).to(device) #our lstm class \n",
        "\n",
        "# loss_fn = nn.MSELoss(reduction=\"mean\")\n",
        "# optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
        "# opt.train(train, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)\n",
        "# opt.plot_losses()\n",
        "\n",
        "# predictions, values = opt.evaluate(test_loader_one, batch_size=1, n_features=input_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.967655Z",
          "start_time": "2021-11-22T21:29:29.967655Z"
        },
        "id": "KHosOOtp6kpV"
      },
      "source": [
        "def inverse_transform(scaler, df, columns):\n",
        "    for col in columns:\n",
        "        df[col] = scaler.inverse_transform(df[col])\n",
        "    return df\n",
        "\n",
        "\n",
        "def format_predictions(predictions, values, df_test, scaler):\n",
        "    vals = np.concatenate(values, axis=0).ravel()\n",
        "    preds = np.concatenate(predictions, axis=0).ravel()\n",
        "    df_result = pd.DataFrame(data={\"value\": vals, \"prediction\": preds})\n",
        "    df_result = df_result.sort_index()\n",
        "    df_result = inverse_transform(scaler, df_result, [[\"value\", \"prediction\"]])\n",
        "    return df_result\n",
        "\n",
        "\n",
        "df_result = format_predictions(predictions, values, X_test, mm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.968653Z",
          "start_time": "2021-11-22T21:29:29.968653Z"
        },
        "id": "dmF9Ym136p6d"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def calculate_metrics(df):\n",
        "    return {'mae' : mean_absolute_error(df.value, df.prediction),\n",
        "            'rmse' : mean_squared_error(df.value, df.prediction) ** 0.5,\n",
        "            'r2' : r2_score(df.value, df.prediction)}\n",
        "\n",
        "result_metrics = calculate_metrics(df_result)\n",
        "result_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.970648Z",
          "start_time": "2021-11-22T21:29:29.970648Z"
        },
        "id": "15bR-HH56r5o"
      },
      "source": [
        "train_predict = model(X_test_tensors.to(device))#forward pass\n",
        "data_predict = train_predict.cpu().data.numpy() #numpy conversion\n",
        "dataY_plot = y_test\n",
        "\n",
        "data_predict = mm.inverse_transform(data_predict) #reverse transformation\n",
        "dataY_plot = mm.inverse_transform(dataY_plot)\n",
        "plt.figure(figsize=(10,6)) #plotting\n",
        "# plt.axvline(x=100000, c='r', linestyle='--') #size of the training set\n",
        "\n",
        "plt.plot(dataY_plot, label='Actuall Data') #actual plot\n",
        "plt.plot(data_predict, label='Predicted Data') #predicted plot\n",
        "plt.title('Time-Series Prediction')\n",
        "plt.legend()\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.970648Z",
          "start_time": "2021-11-22T21:29:29.970648Z"
        },
        "id": "blOZq_zeN3-P"
      },
      "source": [
        "fig , axs = plt.subplots(6, 4, figsize=(32, 40))\n",
        "axs = axs.flatten()\n",
        "i = 0\n",
        "for i, ax in zip(list(range(25)), axs):\n",
        "    ax.plot(dataY_plot[i*1000:(i+1)*1000], label='Actuall Data', alpha=0.7)\n",
        "    ax.plot(data_predict[i*1000:(i+1)*1000], label='Predicted Data') #predicted plot\n",
        "\n",
        "    # ax.axis('off')\n",
        "    ax.title.set_text(f'iter: {i}')\n",
        "    i += 1\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IXApLESOi5w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhX335-YoSXn"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NIfqxMFoUhI"
      },
      "source": [
        "checkpoint = {'model_params': model_params,\n",
        "              'state_dict': model.state_dict()}\n",
        "\n",
        "torch.save(checkpoint, 'checkpoint_case3.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWYD_HyRCUrL"
      },
      "source": [
        "# Load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfq5zmwBCw9o"
      },
      "source": [
        "# chk = torch.load('checkpoint_case2.pth')\n",
        "# print(chk.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3_rpJ4rDDTe"
      },
      "source": [
        "# model.load_state_dict(chk['state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhUA7_XN-Kyx"
      },
      "source": [
        "# Explanations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-PRfRx1-bjn"
      },
      "source": [
        "# import shap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "occYhFlv_J5T"
      },
      "source": [
        "# X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RClsn0u9-7zW"
      },
      "source": [
        "# e = shap.DeepExplainer(\n",
        "#         model, \n",
        "#         X_train_tensors.to(device)\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCIKRaJx_1aA"
      },
      "source": [
        "# shap_values = e.shap_values(\n",
        "#     X_train_tensors[1:10].to(device)\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P648s8UY_11b"
      },
      "source": [
        "# df = pd.DataFrame({\n",
        "#     \"mean_abs_shap\": np.mean(np.abs(shap_values), axis=0), \n",
        "#     \"stdev_abs_shap\": np.std(np.abs(shap_values), axis=0), \n",
        "#     \"name\": final_cols\n",
        "# })\n",
        "# df.sort_values(\"mean_abs_shap\", ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XVs6NJLA5w_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmDbeQySpGwL"
      },
      "source": [
        "# Daily prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGenQ6w8C-fl"
      },
      "source": [
        "df = pd.read_csv(datasets_path+'Day2NWP_C3.csv')\n",
        "df.Date_Time = pd.to_datetime(df.Date_Time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pQxvmJ0-NIY"
      },
      "source": [
        "df.rename(columns={'Speed_10m':'Speed_10m_nwp', 'Speed_50m':'Speed_50m_nwp', 'Direction_50m':'Direction_50m_nwp'},inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMeKE-qI9ZKZ"
      },
      "source": [
        "# final_cols = ['Speed_100m', 'Direction_100m','Air Density_100m','Speed_50m', 'Direction_50m','Air Density_50m','Speed_10m_nwp']\n",
        "X_pred = df[final_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.957682Z",
          "start_time": "2021-11-22T21:29:29.957682Z"
        },
        "id": "etcEaHiq98QL"
      },
      "source": [
        "X_ss_pred = ss.transform(X_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.958679Z",
          "start_time": "2021-11-22T21:29:29.958679Z"
        },
        "id": "yV4aTMey98QL"
      },
      "source": [
        "X_seq_pred = []\n",
        "for i in range(0, X_ss_pred.shape[0]):\n",
        "    if i >= sequence_length:\n",
        "        i_start = i - sequence_length + 1\n",
        "        x = X_ss_pred[i_start:(i + 1), :]\n",
        "        X_seq_pred.append(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T21:29:29.959678Z",
          "start_time": "2021-11-22T21:29:29.959678Z"
        },
        "id": "JaK5ZvwF98QL"
      },
      "source": [
        "X_seq_pred = np.asarray(X_seq_pred)[-96:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJLYD1Mc-g6h"
      },
      "source": [
        "X_seq_pred.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K9dF9iu--Mc"
      },
      "source": [
        "X_pred_tensor = Variable(torch.Tensor(X_seq_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf2lIA_l_pwO"
      },
      "source": [
        "train_predict = model(X_pred_tensor.to(device))\n",
        "data_predict = train_predict.cpu().data.numpy() #numpy conversion\n",
        "data_predict = mm.inverse_transform(data_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tab79ry_wjD"
      },
      "source": [
        "res = pd.DataFrame(data_predict, columns=['power_pred_c3'])\n",
        "res.set_index(df['Date_Time'][-96:],inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nun7QVAeCQsT"
      },
      "source": [
        "res.to_csv('day2_case3_solobolo.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUV_CSx8HPOe"
      },
      "source": [
        "res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_mAEh5211qa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}